{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m environment at `e:\\Pythonfiles\\JP\\ML\\Project.toml`\n"
     ]
    }
   ],
   "source": [
    "] activate ..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: could not import DistributionsAD._mv_categorical_logpdf into ReverseDiffX\n"
     ]
    }
   ],
   "source": [
    "using DifferentialEquations\n",
    "using LinearAlgebra, DiffEqSensitivity, Optim\n",
    "using Flux: flatten, params\n",
    "using DiffEqFlux, Flux\n",
    "using Plots\n",
    "using Flux: train!\n",
    "using NNlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1. Dense()\n",
    "\n",
    "One need to learn array first\n",
    "\n",
    "## 1.1 Dense(1,1)\n",
    "\n",
    "Define, input, weight, loss, opt, train!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function l(x)\n",
    "    2x+1\n",
    "end\n",
    "x = Array(0:0.01:3)'\n",
    "y = l.(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input should be  a Column vector. short to say, input should be a column tensor.  input default is row. output is column tensor\n",
    "\n",
    "weight, bias, params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_nn = Dense(1,1) # define layer\n",
    "test_nn.weight #  weight output\n",
    "test_nn.bias # bias output\n",
    "p0 = Float64.(initial_params(test_nn)) # initial params\n",
    "params(test_nn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can destructure the parameters of Chain, and redefine it and then restructure. For example, choose a new initializer. One can define initializer in layer.\n",
    "\n",
    "For more details, can be seen in Flux src/utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Chain(Dense(5,1,relu,init = Flux.glorot_uniform),Dense(1,2))\n",
    "θ, re = Flux.destructure(model)\n",
    "θ = Flux.glorot_uniform(10)\n",
    "re(θ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_nn(0.1) # input a constant. error\n",
    "test_nn([0.1]) # no error\n",
    "test_nn([0.1 0.2])\n",
    "#test_nn(0:0.1:2) # error,\\\n",
    "x0=Array(0:1:2)' # 0:0.1:2 is a list, should be converted to array, and transpose\n",
    "x0 =[0 1 2]\n",
    "#x0==[0 1 2]\n",
    "test_nn(x0) # true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define opt loss and train! If x y satisfies column principle. loss should not be broadcasted.\n",
    "\n",
    "data should be couple of [(x,y)] x should be column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(a,b) = Flux.Losses.mse(test_nn(a), b)\n",
    "loss(x,y)\n",
    "opt=Descent(0.1)\n",
    "train!(loss, params(test_nn), [(x,y)], opt)\n",
    "loss(x,y)\n",
    "\n",
    "for epoch in 1:1000\n",
    "    train!(loss, params(test_nn), [(x,y)], opt)\n",
    "end\n",
    "params(test_nn)\n",
    "loss(x,y)\n",
    "plot(x', test_nn(x)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can try different:\n",
    "* parameter initializer\n",
    "* layers: hidden layer, activation function\n",
    "* loss function\n",
    "* opt\n",
    "* batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Dense(2,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function ff(x,y)\n",
    "    x*y\n",
    "end\n",
    "x1 = Array(0:0.01:3)'# should be column\n",
    "y1 = Array(4:0.01:7)'\n",
    "z1 = ff.(x1,y1)\n",
    "size(z1)\n",
    "test_nn1= Dense(2,1)\n",
    "xx = [x1;y1] # \n",
    "println(size(xx))\n",
    "test_nn1(xx)\n",
    "#[test_nn1([i[1],i[2]])[1] for i=zip(xx[1], xx[2])]\n",
    "loss1(a,b)=Flux.Losses.mae(test_nn1(a), b)\n",
    "loss1(xx,z1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=ADAM(0.01)\n",
    "\n",
    "train!(loss1, params(test_nn1), [(xx,z1)], opt)\n",
    "println(loss1(xx,z1))\n",
    "\n",
    "for epoch in 1:1000\n",
    "    train!(loss1, params(test_nn1), [(xx,z1)], opt)\n",
    "end\n",
    "params(test_nn1)\n",
    "println(loss1(xx,z1))\n",
    "scatter(z1')\n",
    "plot!(test_nn1(xx)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss: mse, mae, msle, crossentrophy\n",
    "\n",
    "opt: Descent, ADAM(0.01), Momentum, RMSProp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2. Multilayer\n",
    "\n",
    "layers including:\n",
    "\n",
    "* function\n",
    "* DNN\n",
    "* RNN\n",
    "* other architechture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Data and define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Gennerate Data\n",
    "x = Array(-2π:0.01:2π)'\n",
    "function f(x)\n",
    "    x*(1-x)+ x^3\n",
    "end\n",
    "data = sin.(x)\n",
    "#dnn_model = Chain(Dense(1, 1), x-> cos.(x))\n",
    "#dnn_model = Chain(Dense(1, 128, relu), Dense(128, 1), x-> cos.(x))\n",
    "dnn_model = Chain(Dense(1, 10, swish), Dense(10, 100,swish),Dense(100, 10, swish), Dense(10, 1))\n",
    "# dnn_model = Chain(Dense(1, 32, relu), Dense(32, 1, tanh))\n",
    "data .- dnn_model(x)\n",
    "loss2(a,b) = Flux.Losses.mae(dnn_model(x), data)\n",
    "opt=ADAM(0.02)\n",
    "println(loss2(x, data))\n",
    "train!(loss2, params(dnn_model), [(x,data)], opt)\n",
    "println(loss2(x, data))\n",
    "\n",
    "for epoch in 1:5000\n",
    "    train!(loss2, params(dnn_model), [(x,data)], opt)\n",
    "end\n",
    "println(loss2(x, data))\n",
    "y = Array(-2π:0.1:4π)'\n",
    "scatter(sin.(y)')\n",
    "plot!(dnn_model(y)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fun = Chain(Dense(2,2,relu), x -> [sum(x)], Dense(1,1))\n",
    "model_fun([1.0,2.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Gennerate Data\n",
    "x = Array(0:0.01:1)'\n",
    "function f(x)\n",
    "    0.2x+0.4x^2+0.3x*sin(15x)+0.05cos(50x)\n",
    "end\n",
    "data = f.(x)\n",
    "#dnn_model = Chain(Dense(1, 1), x-> cos.(x))\n",
    "#dnn_model = Chain(Dense(1, 128, relu), Dense(128, 1), x-> cos.(x))\n",
    "dnn_model = Chain(Dense(1, 10, sigmoid), Dense(10, 100,swish),Dense(100, 10, swish), Dense(10, 1))\n",
    "# dnn_model = Chain(Dense(1, 32, relu), Dense(32, 1, tanh))\n",
    "data .- dnn_model(x)\n",
    "loss2(a,b) = Flux.Losses.mae(dnn_model(x), data)\n",
    "opt=ADAM(0.02)\n",
    "println(loss2(x, data))\n",
    "train!(loss2, params(dnn_model), [(x,data)], opt)\n",
    "println(loss2(x, data))\n",
    "\n",
    "for epoch in 1:5000\n",
    "    train!(loss2, params(dnn_model), [(x,data)], opt)\n",
    "end\n",
    "println(loss2(x, data))\n",
    "y = Array(0:0.1:1.3)'\n",
    "scatter(f.(y)')\n",
    "plot!(dnn_model(y)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "loss2(a,b) = Flux.Losses.mae(dnn_model(x), data)\n",
    "opt=ADAM(0.01)\n",
    "println(loss2(x, data))\n",
    "train!(loss2, params(dnn_model), [(x,data)], opt)\n",
    "println(loss2(x, data))\n",
    "\n",
    "for epoch in 1:1000\n",
    "    train!(loss2, params(dnn_model), [(x,data)], opt)\n",
    "end\n",
    "println(loss2(x, data))\n",
    "y = Array(-2π:0.1:4π)'\n",
    "scatter(sin.(y)')\n",
    "plot!(dnn_model(y)')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 Data handling, callbacks and save and reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Gennerate Data\n",
    "x = Array(-2π:0.01:2π)'\n",
    "function f(x)\n",
    "    x*(1-x)+ x^3\n",
    "end\n",
    "data = sin.(x)\n",
    "#dnn_model = Chain(Dense(1, 1), x-> cos.(x))\n",
    "#dnn_model = Chain(Dense(1, 128, relu), Dense(128, 1), x-> cos.(x))\n",
    "dnn_model = Chain(Dense(1, 10, swish), Dense(10, 100,swish),Dense(100, 10, swish), Dense(10, 1))\n",
    "# dnn_model = Chain(Dense(1, 32, relu), Dense(32, 1, tanh))\n",
    "data .- dnn_model(x)\n",
    "loss2(a,b) = Flux.Losses.mae(dnn_model(x), data)\n",
    "opt=ADAM(0.02)\n",
    "println(loss2(x, data))\n",
    "train!(loss2, params(dnn_model), [(x,data)], opt)\n",
    "println(loss2(x, data))\n",
    "\n",
    "\n",
    "\n",
    "for epoch in 1:5000\n",
    "    train!(loss2, params(dnn_model), [(x,data)], opt)\n",
    "    if epoch%500==0\n",
    "    println(loss2(x, data))\n",
    "    end\n",
    "end\n",
    "println(loss2(x, data))\n",
    "y = Array(-2π:0.1:4π)'\n",
    "scatter(sin.(y)')\n",
    "plot!(dnn_model(y)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save and load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BSON: @save\n",
    "using BSON: @load\n",
    "@save \"sin_learn_model.bason\" dnn_model\n",
    "@load \"sin_learn_model.bason\" dnn_model\n",
    "dnn_model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4 ODE as an layer\n",
    "the essence is regarding ode solution as a function.\n",
    "\n",
    "f(u_0, p)\n",
    "\n",
    "Input can be u_0 or p. and output of former layer can be input of ode (parameter or initialvalue)\n",
    "\n",
    "the output can be timeseries or end value. Format should be row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function SIR!(du,u,p,t)\n",
    "    Λ, d, β, γ = p\n",
    "    S, I = u\n",
    "    du[1] = Λ - β*S*I - d*S\n",
    "    du[2] = β*S*I - (d+γ)*I\n",
    "end\n",
    "u_0 = [10.0, 0.1]\n",
    "p_data = [1,0.1,0.3,0.1]\n",
    "tspan_data = (0.0, 2.0)\n",
    "function solution_SIR(p)\n",
    "    prob_data = ODEProblem(SIR!,p,tspan_data,p_data)\n",
    "    data_solve = solve(prob_data, Tsit5(),abstol=1e-12, reltol=1e-12, saveat = 0.1)\n",
    "end\n",
    "println(solution_SIR(u_0)[:,end]')\n",
    "\n",
    "function f(x)\n",
    "    2*x+1\n",
    "end\n",
    "#coupled_model = Chain(Dense(2, 21, relu), p -> solution_SIR(p)[1,:], Dense(21,5, tanh),Dense(5, 1))\n",
    "coupled_model = Chain(Dense(2, 2, relu), p -> solution_SIR(p)[1,:], x -> x[2:end], Dense(20,5, tanh),Dense(5, 1))\n",
    "#coupled_model = FastChain(FastDense(2, 2, relu),x -> f.(x), p -> solution_SIR(p)[:,end]', FastDense(2,5, tanh),FastDense(5, 1))\n",
    "#coupled_model = Chain(Dense(2, 2, relu),x -> f.(x), p -> solution_SIR(p)[:,end], Dense(2,5, tanh),Dense(5, 1))\n",
    "\n",
    "u_1 = [5, 0.1]\n",
    "#p_init = Float64.(initial_params(coupled_model))\n",
    "println(coupled_model(u_1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4 DiffEqFlux\n",
    "\n",
    "DiffEqFlux (FastChain) 和 Flux (Chain) 的 区别在于。 Flux中参数都有默认值，除非重新用Flux.params()申明是参数。所以里边默认值，想让他变成参数，只用在Flux.train!()参数中，用Flux.params()包起来。\n",
    "\n",
    "DiffEqFlux里边算子是算子，参数是参数，可以理解为destructure中的re。各有各的好处\n",
    "\n",
    "## 4.1 FastDense(1,1)\n",
    "\n",
    "Note here that FastDense is only operator and eat (data, parameter). Dense eat data. FastDense can eat constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function l(x)\n",
    "    2x+1\n",
    "end\n",
    "x = Array(0:0.01:3)'\n",
    "y = l.(x)\n",
    "\n",
    "test_nn = FastDense(1,1) # define layer\n",
    "params(test_nn) #  weight output\n",
    "#test_nn.bias # bias output\n",
    "p0 = Float64.(initial_params(test_nn)) # initial params\n",
    "params(test_nn)\n",
    "test_nn(x,p0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_nn(0.1) # input a constant. error\n",
    "test_nn(0.1,p0) # no error\n",
    "test_nn([0.1 0.2],p0)\n",
    "#test_nn(0:0.1:2) # error,\\\n",
    "x0=Array(0:1:2)' # 0:0.1:2 is a list, should be converted to array, and transpose\n",
    "x0 =[0 1 2]\n",
    "#x0==[0 1 2]\n",
    "test_nn(x0,p0) # true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = FastChain(FastDense(2,1,swish),FastDense(1,2))\n",
    "model_test1 = FastChain(FastDense(2,1,swish),FastDense(1,2))\n",
    "model_test2 = FastChain(model_test, model_test1)\n",
    "u0 = [0.1,0.2]\n",
    "p = Float64.(initial_params(model_test1))\n",
    "p_0 = [0.1,0.2,0.3,0.4,0.5,0.6,0.7]\n",
    "print(p)\n",
    "model_test1(u0,[p;p_0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function train(θ)\n",
    "    test_nn(x,θ)\n",
    "end\n",
    "\n",
    "\n",
    "# loss, keep the same\n",
    "\n",
    "function loss(θ)\n",
    "    pred = train(θ)\n",
    "    sum(abs2, (y .- pred)), pred # + 1e-5*sum(sum.(abs, params(ann)))\n",
    "end\n",
    "\n",
    "# callback, keep  the same\n",
    "const losses = []\n",
    "callback(θ,l,pred) = begin\n",
    "    push!(losses, l)\n",
    "    if length(losses)%50==0\n",
    "        println(losses[end])\n",
    "    end\n",
    "    false\n",
    "end\n",
    "\n",
    "res = DiffEqFlux.sciml_train(loss, p0, ADAM(0.01), cb=callback, maxiters = 1000)\n",
    "#res2_node = DiffEqFlux.sciml_train(loss, res1_node.minimizer, BFGS(initial_stepnorm=0.01), cb=callback, maxiters = 1000)\n",
    "loss(res.minimizer)\n",
    "plot(x', test_nn(x,res.minimizer)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Multilayer\n",
    "\n",
    "function layer should be (x,p) -> f.(x, p). x is output of former layer, p is parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Gennerate Data\n",
    "x = Array(-2π:0.01:2π)'\n",
    "function f(x)\n",
    "    x*(1-x)+ x^3\n",
    "end\n",
    "data = sin.(x)\n",
    "#dnn_model = Chain(Dense(1, 1), x-> cos.(x))\n",
    "#dnn_model = Chain(Dense(1, 128, relu), Dense(128, 1), x-> cos.(x))\n",
    "dnn_model = FastChain(FastDense(1, 10, swish), FastDense(10, 100,swish),FastDense(100, 10, swish), FastDense(10, 1), (x,p)->cos.(x))\n",
    "# dnn_model = Chain(Dense(1, 32, relu), Dense(32, 1, tanh))\n",
    "p = Float64.(initial_params(dnn_model)) \n",
    "\n",
    "function train(θ)\n",
    "    dnn_model(x,θ)\n",
    "end\n",
    "\n",
    "\n",
    "# loss, keep the same\n",
    "\n",
    "function loss(θ)\n",
    "    pred = train(θ)\n",
    "    sum(abs2, (data .- pred)), pred # + 1e-5*sum(sum.(abs, params(ann)))\n",
    "end\n",
    "\n",
    "# callback, keep  the same\n",
    "const losses = []\n",
    "callback(θ,l,pred) = begin\n",
    "    push!(losses, l)\n",
    "    if length(losses)%50==0\n",
    "        println(losses[end])\n",
    "    end\n",
    "    false\n",
    "end\n",
    "\n",
    "res = DiffEqFlux.sciml_train(loss, p, ADAM(0.01), cb=callback, maxiters = 1000)\n",
    "#res2_node = DiffEqFlux.sciml_train(loss, res1_node.minimizer, BFGS(initial_stepnorm=0.01), cb=callback, maxiters = 1000)\n",
    "loss(res.minimizer)\n",
    "y = Array(-2π:0.1:4π)'\n",
    "plot(y', dnn_model(y,res.minimizer)')\n",
    "scatter!(y', sin.(y)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 NeuralODE as a layer\n",
    "\n",
    "### 5.3.1 method one， solution map as a function\n",
    "\n",
    "remember that for DiffEqFlux, function should be (x,p) -> f(x).\n",
    "\n",
    "Thus, ode as a function in this method. One can not estimate parameters but can use parameter as output of upper layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function SIR!(du,u,p,t)\n",
    "    Λ, d, β, γ = p\n",
    "    S, I = u\n",
    "    du[1] = Λ - β*S*I - d*S\n",
    "    du[2] = β*S*I - (d+γ)*I\n",
    "end\n",
    "u_0 = [10.0, 0.1]\n",
    "p_data = [1,0.1,0.3,0.1]\n",
    "tspan_data = (0.0, 2.0)\n",
    "function solution_SIR(u)\n",
    "    prob_data = ODEProblem(SIR!,u,tspan_data,p_data)\n",
    "    data_solve = solve(prob_data, Tsit5(),abstol=1e-12, reltol=1e-12, saveat = 0.1)\n",
    "end\n",
    "println(solution_SIR(u_0)[:,end]')\n",
    "\n",
    "function f(x)\n",
    "    2*x+1\n",
    "end\n",
    "#coupled_model = Chain(Dense(2, 21, relu), p -> solution_SIR(p)[1,:], Dense(21,5, tanh),Dense(5, 1))\n",
    "coupled_model = FastChain(FastDense(2, 2, relu), (x, p) -> solution_SIR(x)[1,:], (x, p) -> x[2:end], FastDense(20,5, tanh),FastDense(5, 1))\n",
    "#coupled_model = FastChain(FastDense(2, 2, relu),x -> f.(x), p -> solution_SIR(p)[:,end]', FastDense(2,5, tanh),FastDense(5, 1))\n",
    "#coupled_model = Chain(Dense(2, 2, relu),x -> f.(x), p -> solution_SIR(p)[:,end], Dense(2,5, tanh),Dense(5, 1))\n",
    "\n",
    "u_1 = [5, 0.1]\n",
    "p_init = Float64.(initial_params(coupled_model))\n",
    "println(coupled_model(u_1,p_init))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2 Method two: Neural ODE\n",
    "\n",
    "for this kind, former layer input should be initial value of neuralode, parameters should be layer parameter. output are sequence.\n",
    "\n",
    "NeuralODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "u0 = Float32[2.0; 0.0]\n",
    "datasize = 30\n",
    "tspan = (0.0f0, 1.5f0)\n",
    "tsteps = range(tspan[1], tspan[2], length = datasize)\n",
    "\n",
    "function trueODEfunc(du, u, p, t)\n",
    "    true_A = [-0.1 2.0; -2.0 -0.1]\n",
    "    du .= ((u.^3)'true_A)'\n",
    "end\n",
    "\n",
    "prob_trueode = ODEProblem(trueODEfunc, u0, tspan)\n",
    "ode_data = Array(solve(prob_trueode, Tsit5(), saveat = tsteps))\n",
    "\n",
    "dudt2 = FastChain((x, p) -> x.^3,\n",
    "                  FastDense(2, 50, tanh),\n",
    "                  FastDense(50, 2))\n",
    "model1 = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps)\n",
    "\n",
    "#model1 = FastChain(FastDense(2,2,swish), nn_ode, FastDense(21,1,relu))\n",
    "#p1 = Float64.(initial_params(model1))\n",
    "model1(u0,model1.p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function SIR!(u,p)\n",
    "    Λ, d, β, γ = p\n",
    "    S, I = u\n",
    "    du[1] = Λ - β*S*I - d*S\n",
    "    du[2] = β*S*I - (d+γ)*I\n",
    "    [du[1],du[2]]\n",
    "end\n",
    "u_0 = [10.0, 0.1]\n",
    "p_data = [1,0.1,0.3,0.1]\n",
    "tspan_data = (0.0f0, 2.0f0)\n",
    "\n",
    "chain_1 = FastChain(FastDense(2,21,swish), FastDense(21,2,relu))\n",
    "\n",
    "model2 = NeuralODE(chain_1, tspan_data, Tsit5(), saveat = 0.1)\n",
    "\n",
    "#model1 = FastChain(FastDense(2,2,swish), nn_ode, FastDense(21,1,relu))\n",
    "#p1 = Float64.(initial_params(model1))\n",
    "model2(u_0,model2.p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6 DiffEqFlux Basis\n",
    "\n",
    "## 6.1 Autodifferentialation\n",
    "\n",
    "Zygote\n",
    "\n",
    "ReverseDiff\n",
    "\n",
    "ForwardDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DiffEqSensitivity, OrdinaryDiffEq, Zygote\n",
    "\n",
    "function fiip(du,u,p,t)\n",
    "  du[1] = dx = p[1]*u[1] - p[2]*u[1]*u[2]\n",
    "  du[2] = dy = -p[3]*u[2] + p[4]*u[1]*u[2]\n",
    "end\n",
    "p = [1.5,1.0,3.0,1.0]; u0 = [1.0;1.0]\n",
    "prob = ODEProblem(fiip,u0,(0.0,10.0),p)\n",
    "sol = solve(prob,Tsit5())\n",
    "loss(u0,p) = sum(solve(prob,Tsit5(),u0=u0,p=p,saveat=0.1))\n",
    "du01,dp1 = Zygote.gradient(loss,u0,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also adjoint method\n",
    "\n",
    "ReverseDiffAdjoint\n",
    "\n",
    "QuadratureAdjoint\n",
    "\n",
    "InterpolatingAdjoint\n",
    "\n",
    "The following algorithm choices exist for sensealg. See the sensitivity mathematics page for more details on the definition of the methods.\n",
    "\n",
    "* ForwardSensitivity(;ADKwargs...): An implementation of continuous forward sensitivity analysis for propagating derivatives by solving the   extended ODE. Only supports ODEs.\n",
    "* ForwardDiffSensitivity(;chunk_size=0,convert_tspan=true): An implementation of discrete forward sensitivity analysis through ForwardDiff.jl. This algorithm can differentiate code with callbacks when convert_tspan=true, but will be faster when convert_tspan=false.\n",
    "* BacksolveAdjoint(;checkpointing=true,ADKwargs...): An implementation of adjoint sensitivity analysis using a backwards solution of the ODE. By default this algorithm will use the values from the forward pass to perturb the backwards solution to the correct spot, allowing reduced memory with stabilization. Only supports ODEs and SDEs.\n",
    "* InterpolatingAdjoint(;checkpointing=false;ADKwargs...): The default. An implementation of adjoint sensitivity analysis which uses the interpolation of the forward solution for the reverse solve vector-Jacobian products. By default it requires a dense solution of the forward pass and will internally ignore saving arguments during the gradient calculation. When checkpointing is enabled it will only require the memory to interpolate between checkpoints. Only supports ODEs and SDEs.\n",
    "* QuadratureAdjoint(;abstol=1e-6,reltol=1e-3,compile=false,ADKwargs...): An implementation of adjoint sensitivity analysis which develops a full continuous solution of the reverse solve in order to perform a post-ODE quadrature. This method requires the the dense solution and will ignore saving arguments during the gradient calculation. The tolerances in the constructor control the inner quadrature. The inner quadrature uses a ReverseDiff vjp if autojacvec, and compile=false by default but can compile the tape under the same circumstances as ReverseDiffVJP. Only supports ODEs.\n",
    "* ReverseDiffAdjoint(): An implementation of discrete adjoint sensitivity analysis using the ReverseDiff.jl tracing-based AD. Supports in-place functions through an Array of Structs formulation, and supports out of place through struct of arrays.\n",
    "* TrackerAdjoint(): An implementation of discrete adjoint sensitivity analysis using the Tracker.jl tracing-based AD. Supports in-place functions through an Array of Structs formulation, and supports out of place through struct of arrays.\n",
    "* ZygoteAdjoint(): An implementation of discrete adjoint sensitivity analysis using the Zygote.jl source-to-source AD directly on the differential equation solver. Currently fails.\n",
    "* SensitivityADPassThrough(): Ignores all adjoint definitions and proceeds to do standard AD through the solve functions.\n",
    "* ForwardLSS(), AdjointLSS(), and NILSS(nseg,nstep): Implementation of shadowing methods for chaotic systems with a long-time averaged objective. See the sensitivity analysis for chaotic systems (shadowing methods) section for more details.\n",
    "\n",
    "\n",
    "The ReverseDiffAdjoint(), TrackerAdjoint(), ZygoteAdjoint(), and SensitivityADPassThrough() algorithms all offer differentiate-through-the-solver adjoints, each based on their respective automatic differentiation packages. If you're not sure which to use, ReverseDiffAdjoint() is generally a stable and performant best if using the CPU, while TrackerAdjoint() is required if you need GPUs. Note that SensitivityADPassThrough() is more or less an internal implementation detail. For example, ReverseDiffAdjoint() is implemented by invoking ReverseDiff's AD functionality on solve(...; sensealg=SensitivityADPassThrough())."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sum_of_solution(u0,p)\n",
    "    _prob = remake(prob,u0=u0,p=p)\n",
    "    sum(solve(_prob,Tsit5(),rtol=1e-6,atol=1e-6,saveat=0.1,sensealg=QuadratureAdjoint()))\n",
    "end\n",
    "du01,dp1 = Zygote.gradient(sum_of_solution,u0,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2 Train ODE\n",
    "\n",
    "## DiffEqFlux.sciml_train\n",
    "—\n",
    "Function\n",
    "sciml_train\n",
    "\n",
    "* Unconstrained Optimization\n",
    "```\n",
    "function sciml_train(loss, _θ, opt = DEFAULT_OPT, adtype = DEFAULT_AD,\n",
    "                     _data = DEFAULT_DATA, args...;\n",
    "                     cb = (args...) -> false, maxiters = get_maxiters(data),\n",
    "                     kwargs...)\n",
    "```\n",
    "* Box Constrained Optimization\n",
    "```\n",
    "function sciml_train(loss, θ, opt = DEFAULT_OPT, adtype = DEFAULT_AD,\n",
    "                     data = DEFAULT_DATA, args...;\n",
    "                     lower_bounds, upper_bounds,\n",
    "                     cb = (args...) -> (false), maxiters = get_maxiters(data),\n",
    "                     kwargs...)\n",
    "```\n",
    "* Optimizer Choices and Arguments (GalacticOptim.jl)\n",
    "\n",
    "For a full definition of the allowed optimizers and arguments, please see the GalacticOptim.jl documentation. As sciml_train is an interface over GalacticOptim.jl, all of its optimizers and arguments can be used from here.\n",
    "\n",
    "* Loss Functions and Callbacks\n",
    "\n",
    "Loss functions in sciml_train treat the first returned value as the return. For example, if one returns (1.0, [2.0]), then the value the optimizer will see is 1.0. The other values are passed to the callback function. The callback function is cb(p, args...) where the arguments are the extra returns from the loss. This allows for reusing instead of recalculating. The callback function must return a boolean where if true, then the optimizer will prematurely end the optimization. It is called after every successful step, something that is defined in an optimizer-dependent manner.\n",
    "\n",
    "* Default AD Choice\n",
    "\n",
    "The current default AD choice is dependent on the number of parameters, where for <100 parameters ForwardDiff.jl is used, otherwise Zygote.jl is used. More refinements to the techniques are planned.\n",
    "\n",
    "* Default Optimizer Choice\n",
    "\n",
    "By default, if the loss function is deterministic than an optimizer chain of ADAM -> BFGS is used, otherwise ADAM is used (and a choice of maxiters is required)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DifferentialEquations, DiffEqFlux, Plots\n",
    "\n",
    "function lotka_volterra!(du, u, p, t)\n",
    "  x, y = u\n",
    "  α, β, δ, γ = p\n",
    "  du[1] = dx = α*x - β*x*y\n",
    "  du[2] = dy = -δ*y + γ*x*y\n",
    "end\n",
    "\n",
    "# Initial condition\n",
    "u0 = [1.0, 1.0]\n",
    "\n",
    "# Simulation interval and intermediary points\n",
    "tspan = (0.0, 10.0)\n",
    "tsteps = 0.0:0.1:10.0\n",
    "\n",
    "# LV equation parameter. p = [α, β, δ, γ]\n",
    "p = [1.5, 1.0, 3.0, 1.0]\n",
    "\n",
    "# Setup the ODE problem, then solve\n",
    "prob = ODEProblem(lotka_volterra!, u0, tspan, p)\n",
    "sol = solve(prob, Tsit5())\n",
    "\n",
    "# Plot the solution\n",
    "using Plots\n",
    "plot(sol)\n",
    "\n",
    "function loss(p)\n",
    "  sol = solve(prob, Tsit5(), p=p, saveat = tsteps)\n",
    "  loss = sum(abs2, sol.-1)\n",
    "  return loss, sol\n",
    "end\n",
    "\"\"\"\n",
    "callback = function (p, l, pred)\n",
    "  display(l)\n",
    "  plt = plot(pred, ylim = (0, 6))\n",
    "  display(plt)\n",
    "  # Tell sciml_train to not halt the optimization. If return true, then\n",
    "  # optimization stops.\n",
    "  return false\n",
    "end\n",
    "\"\"\"\n",
    "result_ode = DiffEqFlux.sciml_train(loss, p,\n",
    "                                    #cb = callback,\n",
    "                                    lower_bounds = (1, 0, 2.0, 0),\n",
    "                                    upper_bounds = (5, 4, 4.0, 3),\n",
    "                                    maxiters = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DiffEqFlux, DifferentialEquations, LinearAlgebra\n",
    "k, α, β, γ = 1, 0.1, 0.2, 0.3\n",
    "tspan = (0.0,10.0)\n",
    "\n",
    "function dxdt_train(du,u,p,t)\n",
    "  du[1] = u[2]\n",
    "  du[2] = -k*u[1] - α*u[1]^3 - β*u[2] - γ*u[2]^3\n",
    "end\n",
    "\n",
    "u0 = [1.0,0.0]\n",
    "ts = collect(0.0:0.1:tspan[2])\n",
    "prob_train = ODEProblem{true}(dxdt_train,u0,tspan,p=nothing)\n",
    "data_train = Array(solve(prob_train,Tsit5(),saveat=ts))\n",
    "\n",
    "\n",
    "A = [LegendreBasis(10), LegendreBasis(10)]\n",
    "nn = TensorLayer(A, 1)\n",
    "f = x -> min(30one(x),x)\n",
    "\n",
    "function dxdt_pred(du,u,p,t)\n",
    "  du[1] = u[2]\n",
    "  du[2] = -p[1]*u[1] - p[2]*u[2] + f.(nn(u,p[3:end])[1])\n",
    "end\n",
    "\n",
    "α = zeros(102)\n",
    "\n",
    "prob_pred = ODEProblem{true}(dxdt_pred,u0,tspan,p=nothing)\n",
    "\n",
    "function predict_adjoint(θ)\n",
    "    x = Array(solve(prob_pred,Tsit5(),p=θ,saveat=ts))\n",
    "end\n",
    "  \n",
    "function loss_adjoint(θ)\n",
    "    x = predict_adjoint(θ)\n",
    "    loss = sum(norm.(x - data_train))\n",
    "    return loss\n",
    "end\n",
    "  \n",
    "function cb(θ,l)\n",
    "@show θ, l\n",
    "return false\n",
    "end\n",
    "\n",
    "res1 = DiffEqFlux.sciml_train(loss_adjoint, α, ADAM(0.05), cb = cb, maxiters = 150)\n",
    "res2 = DiffEqFlux.sciml_train(loss_adjoint, res1.u, ADAM(0.001), cb = cb,maxiters = 150)\n",
    "opt = res2.u\n",
    "\n",
    "using Plots\n",
    "data_pred = predict_adjoint(opt)\n",
    "plot(ts, data_train[1,:], label = \"X (ODE)\")\n",
    "plot!(ts, data_train[2,:], label = \"V (ODE)\")\n",
    "plot!(ts, data_pred[1,:], label = \"X (NN)\")\n",
    "plot!(ts, data_pred[2,:],label = \"V (NN)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDE 和 ODE 参数花式共估计\n",
    "本例子中，ODE的初值，参数也可以估计。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DiffEqFlux, DifferentialEquations, Plots\n",
    "\n",
    "u0 = 1.1f0\n",
    "tspan = (0.0f0, 25.0f0)\n",
    "tsteps = 0.0f0:1.0:25.0f0\n",
    "\n",
    "model_univ = FastChain(FastDense(2, 16, tanh),\n",
    "                       FastDense(16, 16, tanh),\n",
    "                       FastDense(16, 1))\n",
    "\n",
    "# The model weights are destructured into a vector of parameters\n",
    "p_model = initial_params(model_univ)\n",
    "n_weights = length(p_model)\n",
    "\n",
    "# Parameters of the second equation (linear dynamics)\n",
    "p_system = Float32[0.5, -0.5]\n",
    "\n",
    "p_all = [p_model; p_system]\n",
    "θ = Float32[u0; p_all]\n",
    "\n",
    "function dudt_univ!(du, u, p, t)\n",
    "    # Destructure the parameters\n",
    "    model_weights = p[1:n_weights]\n",
    "    α = p[end - 1]\n",
    "    β = p[end]\n",
    "\n",
    "    # The neural network outputs a control taken by the system\n",
    "    # The system then produces an output\n",
    "    model_control, system_output = u\n",
    "\n",
    "    # Dynamics of the control and system\n",
    "    dmodel_control = model_univ(u, model_weights)[1]\n",
    "    dsystem_output = α*system_output + β*model_control\n",
    "\n",
    "    # Update in place\n",
    "    du[1] = dmodel_control\n",
    "    du[2] = dsystem_output\n",
    "end\n",
    "\n",
    "prob_univ = ODEProblem(dudt_univ!, [0f0, u0], tspan, p_all)\n",
    "sol_univ = solve(prob_univ, Tsit5(),abstol = 1e-8, reltol = 1e-6)\n",
    "\n",
    "function predict_univ(θ)\n",
    "  return Array(solve(prob_univ, Tsit5(), u0=[0f0, θ[1]], p=θ[2:end],\n",
    "                              sensealg = InterpolatingAdjoint(autojacvec=ReverseDiffVJP(true)),\n",
    "                              saveat = tsteps))\n",
    "end\n",
    "\n",
    "loss_univ(x) = sum(abs2, predict_univ(x)[2,:] .- 1)\n",
    "#l = loss_univ(θ)\n",
    "\n",
    "list_plots = []\n",
    "iter = 0\n",
    "\"\"\"\n",
    "function cb(θ,l)\n",
    "  @show θ, l\n",
    "  return false\n",
    "end\n",
    "callback = function (θ, l)\n",
    "  global list_plots, iter\n",
    "\n",
    "  if iter == 0\n",
    "    list_plots = []\n",
    "  end\n",
    "  iter += 1\n",
    "\n",
    "  if iter%50==0\n",
    "\n",
    "  println(l)\n",
    "\n",
    "  plt = plot(predict_univ(θ)', ylim = (0, 6))\n",
    "  push!(list_plots, plt)\n",
    "  display(plt)\n",
    "  end\n",
    "  return false\n",
    "end\n",
    "\"\"\"\n",
    "result_univ = DiffEqFlux.sciml_train(loss_univ, θ)\n",
    "                                     #cb = cb)\n",
    "plot(predict_univ(result_univ.minimizer)', ylim = (0, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7. Tensor Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any[0.024067914020447057, 0.0770612827775583, 0.12570722627111697, 0.16970812760320353, 0.20879077934659707, 0.24270796726385768, 0.2712399092445992, 0.2941955412443673, 0.31141364287290574, 0.3227637961858578, 0.32814717217679545, 0.3274971404406819, 0.3207796984798439, 0.3079937181436583, 0.2891710077275931, 0.2643761893001775, 0.2337063918719533, 0.1972907620625799, 0.1552897949551093, 0.1078944888441371, 0.05532532858135519, -0.002168896807748122, -0.0643124356241015, -0.13080406659944144, -0.2013187352338539, -0.2755093241844586, -0.35300854776427015, -0.43343095990751246, -0.5163750643151953, -0.6014255149161504, -0.6881553942673373, -0.7761285570758356, -0.8649020256562162, -0.9540284238428438, -1.0430584356589234, -1.1315432749038752, -1.2190371517588399, -1.3050997225269403, -1.3892985087204945, -1.4712112718809232, -1.5504283307677742, -1.6265548078796828, -1.6992127926701999, -1.768043409293211, -1.8327087772532353, -1.8928938539423146, -1.9483081487140173, -1.9986872988724096, -2.0437944987357053, -2.0834217737661076, -2.1173910926345303, -2.1455553110063317, -2.1677989417867956, -2.184038747547407, -2.1942241518604257, -2.1983374672941896, -2.19639393885909, -2.1884416027384765, -2.1745609611838375, -2.154864475493633, -2.129495880024158, -2.0986293211930116, -2.0624683264253427, -2.0212446089544827, -1.9752167153163853, -1.9246685232661693, -1.8699075986900677, -1.811263420882328, -1.7490854862996172, -1.6837413015910994, -1.615614277326558, -1.5451015344043586, -1.4726116356123773, -1.3985622552355526, -1.3233777999510783, -1.2474869945244058, -1.1713204460147177, -1.0953082003162122, -1.0198773049007221, -0.9454493915877643, -0.8724382930501347, -0.801247706567532, -0.7322689182682738, -0.6658786007517472, -0.6024366965636114, -0.542284399504458, -0.48574224519335074, -0.43310832168360913, -0.3846566102429849, -0.3406354656677413, -0.30126624470452934, -0.26674209030962837, -0.23722687858710567, -0.21285433432064416, -0.19372732005350723, -0.1799173026828329, -0.17146400052373434, -0.16837521277131018, -0.1706268322504626, -0.17816304130027383, -0.19089668959752282, -0.20870985168866563, -0.23145456097712314, -0.25895371590882554, -0.2910021531193574, -0.327367881356292, -0.3677934690757919, -0.41199757773846724, -0.45967663200082404, -0.5105066172200283, -0.5641449939656822, -0.6202327185668899, -0.6783963581198625, -0.7382502878441882, -0.7993989582075502, -0.8614392188419995, -0.923962685951909, -0.9865581396664364, -1.0488139376190746, -1.1103204309445447, -1.170672368869602, -1.229471278139136, -1.2863278036620582, -1.3408639969820006, -1.3927155394745032, -1.4415338875435761]\n",
      "29790.728923824285\n",
      "7916.828727866346\n",
      "Any[-6.988936175549715e-5, -7.019781148163984e-5, -7.050603820763293e-5, -7.081404194433927e-5, -7.112182270259393e-5, -7.142938049329965e-5, -7.173671532727764e-5, -7.204382721542196e-5, -7.235071616857637e-5, -7.265738219761585e-5, -7.296382531341364e-5, -7.327004552683258e-5, -7.35760428487442e-5, -7.388181729002866e-5, -7.418736886155228e-5, -7.449269757419351e-5, -7.47978034388308e-5, -7.510268646634781e-5, -7.540734666760911e-5, -7.571178405353651e-5, -7.601599863497031e-5, -7.631999042282539e-5, -7.662375942796978e-5, -7.692730566131664e-5, -7.723062913373574e-5, -7.753372985612982e-5, -7.783660783938946e-5, -7.813926309441395e-5, -7.844169563210601e-5, -7.874390546334237e-5, -7.90458925990483e-5, -7.934765705013176e-5, -7.964919882747466e-5, -7.995051794199015e-5, -8.025161440458443e-5, -8.055248822618627e-5, -8.085313941767411e-5, -8.115356798997671e-5, -8.145377395401068e-5, -8.175375732069785e-5, -8.205351810093922e-5, -8.235305630566354e-5, -8.26523719457805e-5, -8.295146503223445e-5, -8.325033557593682e-5, -8.354898358780248e-5, -8.384740907876366e-5, -8.41456120597682e-5, -8.444359254172751e-5, -8.474135053557728e-5, -8.503888605225322e-5, -8.533619910268755e-5, -8.563328969782465e-5, -8.593015784859846e-5, -8.62268035659395e-5, -8.652322686080252e-5, -8.681942774413538e-5, -8.711540622686684e-5, -8.74111623199586e-5, -8.770669603434984e-5, -8.800200738099187e-5, -8.829709637083773e-5, -8.859196301484046e-5, -8.888660732395658e-5, -8.918102930915474e-5, -8.947522898136197e-5, -8.976920635156253e-5, -9.006296143070774e-5, -9.035649422978187e-5, -9.064980475971367e-5, -9.094289303149435e-5, -9.123575905608562e-5, -9.152840284446136e-5, -9.182082440757806e-5, -9.211302375642695e-5, -9.240500090198014e-5, -9.26967558552063e-5, -9.298828862708969e-5, -9.327959922859896e-5, -9.35706876707236e-5, -9.386155396444093e-5, -9.415219812074042e-5, -9.444262015060809e-5, -9.47328200650386e-5, -9.502279787500234e-5, -9.531255359150614e-5, -9.560208722554295e-5, -9.589139878809702e-5, -9.618048829016478e-5, -9.646935574274783e-5, -9.675800115683392e-5, -9.704642454345588e-5, -9.733462591358759e-5, -9.762260527823065e-5, -9.791036264841443e-5, -9.819789803510934e-5, -9.848521144937249e-5, -9.877230290217774e-5, -9.905917240454407e-5, -9.934581996749042e-5, -9.963224560203404e-5, -9.991844931918348e-5, -0.0001002044311299681, -0.00010049019104539474, -0.00010077572907650663, -0.00010106104523429499, -0.00010134613952981694, -0.00010163101197408278, -0.00010191566257812361, -0.000102200091352974, -0.00010248429830965466, -0.00010276828345921057, -0.00010305204681266937, -0.00010333558838107257, -0.00010361890817545821, -0.00010390200620685915, -0.00010418488248632382, -0.00010446753702489028, -0.00010474996983360005, -0.00010503218092351892, -0.00010531417030567626, -0.0001055959379911188, -0.00010587748399091233, -0.00010615880831610532, -0.00010643991097774273, -0.00010672079198688865, -0.0001070014513546106, -0.00010728188909194317, -0.00010756210520997643, -0.0001078420997197415, -0.00010812187263232499, -0.00010840142395879097, -0.00010868075371019831, -0.00010895986189761973, -0.00010923874853213666, -0.00010951741362479755, -0.0001097958571867029, -0.00011007407922890809, -0.00011035207976249455, -0.00011062985879856102, -0.00011090741634815597, -0.00011118475242238857, -0.00011146186703232461, -0.00011173876018905071, -0.0001120154319036639, -0.00011229188218725254, -0.0001125681110509015, -0.00011284411850570955, -0.00011311990456275636, -0.00011339546923314937, -0.00011367081252797694, -0.00011394593445834648, -0.0001142208350353481, -0.0001144955142700875, -0.00011476997217367904, -0.00011504420875721282, -0.00011531822403180146, -0.00011559201800855068, -0.00011586559069856964, -0.00011613894211298312, -0.00011641207226286907, -0.00011668498115938349, -0.00011695766881361994, -0.00011723013523670667, -0.00011750238043975458, -0.00011777440443388326, -0.00011804620723023136, -0.00011831778883990458, -0.00011858914927403985, -0.00011886028854375501, -0.00011913120666018871, -0.00011940190363447616, -0.00011967237947774212, -0.00011994263420110965, -0.00012021266781573822, -0.00012048248033275435, -0.00012075207176328456, -0.00012102144211848659, -0.00012129059140950603, -0.00012155951964746767, -0.00012182822684352751, -0.0001220967130088433, -0.0001223649781545398, -0.0001226330222917956, -0.00012290084543172854, -0.0001231684475855241, -0.0001234358287643244, -0.00012370298897928025, -0.00012396992824156502]"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "Cannot convert Adjoint{Any, Vector{Any}} to series data for plotting",
     "output_type": "error",
     "traceback": [
      "Cannot convert Adjoint{Any, Vector{Any}} to series data for plotting",
      "",
      "Stacktrace:",
      " [1] error(s::String)",
      "   @ Base .\\error.jl:33",
      " [2] _prepare_series_data(x::Adjoint{Any, Vector{Any}})",
      "   @ RecipesPipeline F:\\Code\\Julia\\Julia-1.6.2\\jpkgs\\.julia\\packages\\RecipesPipeline\\i2bA0\\src\\series.jl:8"
     ]
    }
   ],
   "source": [
    "# 1. Gennerate Data from sin(x)\n",
    "x = Array(-2π:0.1:2π)'\n",
    "data = sin.(x)\n",
    "\n",
    "# 2. learn by dnn with a hidden TensorLayer\n",
    "function dnn_output(θ,v)\n",
    "    tensor_layer = TensorLayer([SinBasis(2)], 1)\n",
    "    dnn_model = FastChain(FastDense(1,1),(y,p)-> tensor_layer(y,θ[1:2])[1], FastDense(1, 1))\n",
    "    value = []\n",
    "    for i = v\n",
    "    push!(value,dnn_model(i,θ[3:end])[1])\n",
    "    end\n",
    "    Array(value)\n",
    "end\n",
    "function train(θ)\n",
    "    dnn_output(θ,x)\n",
    "end\n",
    "p = randn(6)\n",
    "println(train(p))  # output a vector.\n",
    "function loss(θ)\n",
    "    pred = train(θ)\n",
    "    sum(abs2, (data .- pred))\n",
    "end\n",
    "println(loss(p)) # 29790\n",
    "res = DiffEqFlux.sciml_train(loss, p, ADAM(0.05), maxiters = 2000)\n",
    "println(loss(res.minimizer)) # 7916"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DiffEqFlux, DifferentialEquations, LinearAlgebra\n",
    "k, α, β, γ = 1, 0.1, 0.2, 0.3\n",
    "tspan = (0.0,10.0)\n",
    "\n",
    "function dxdt_train(du,u,p,t)\n",
    "  du[1] = u[2]\n",
    "  du[2] = -k*u[1] - α*u[1]^3 - β*u[2] - γ*u[2]^3\n",
    "end\n",
    "\n",
    "u0 = [1.0,0.0]\n",
    "ts = collect(0.0:0.1:tspan[2])\n",
    "prob_train = ODEProblem{true}(dxdt_train,u0,tspan,p=nothing)\n",
    "data_train = Array(solve(prob_train,Tsit5(),saveat=ts))\n",
    "\n",
    "A = [LegendreBasis(10), LegendreBasis(10)]\n",
    "nn = TensorLayer(A, 1)\n",
    "\n",
    "f = x -> min(30one(x),x)\n",
    "\n",
    "function dxdt_pred(du,u,p,t)\n",
    "  du[1] = u[2]\n",
    "  du[2] = -p[1]*u[1] - p[2]*u[2] + f(nn(u,p[3:end])[1])\n",
    "end\n",
    "\n",
    "α = zeros(102)\n",
    "\n",
    "prob_pred = ODEProblem{true}(dxdt_pred,u0,tspan,p=nothing)\n",
    "\n",
    "function predict_adjoint(θ)\n",
    "    x = Array(solve(prob_pred,Tsit5(),p=θ,saveat=ts))\n",
    "end\n",
    "\n",
    "function loss_adjoint(θ)\n",
    "x = predict_adjoint(θ)\n",
    "loss = sum(norm.(x - data_train))\n",
    "return loss\n",
    "end\n",
    "\n",
    "function cb(θ,l)\n",
    "@show θ, l\n",
    "return false\n",
    "end\n",
    "res1 = DiffEqFlux.sciml_train(loss_adjoint, α, ADAM(0.05), cb = cb, maxiters = 150)\n",
    "res2 = DiffEqFlux.sciml_train(loss_adjoint, res1.u, ADAM(0.001), cb = cb,maxiters = 150)\n",
    "opt = res2.u\n",
    "using Plots\n",
    "data_pred = predict_adjoint(opt)\n",
    "plot(ts, data_train[1,:], label = \"X (ODE)\")\n",
    "plot!(ts, data_train[2,:], label = \"V (ODE)\")\n",
    "plot!(ts, data_pred[1,:], label = \"X (NN)\")\n",
    "plot!(ts, data_pred[2,:],label = \"V (NN)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float32[0.88244027 1.7648805 2.6473207]\n",
      "[0.0]\n",
      "Float32[-0.967725 -1.93545 -2.9031749]\n"
     ]
    }
   ],
   "source": [
    "using DiffEqFlux, DifferentialEquations, LinearAlgebra\n",
    "B = [PolynomialBasis(2)]\n",
    "nn0 = Dense(1,1)\n",
    "nn1 = TensorLayer(B, 1)\n",
    "nn2 = FastDense(1,1)\n",
    "u = [1 2 3]\n",
    "println(nn0(u)) # Float32[0.95563 1.91126 2.86689]\n",
    "println(nn1(u,zeros(2))) # [-0.9934653167126005]\n",
    "println(nn2(u,initial_params(nn2))) # Float32[-0.7425173 -1.4850346 -2.227552]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.16022876501083375 -0.3204575300216675 -0.4806862831115723]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3-element Vector{Any}:\n",
       " -0.013367999035494904\n",
       " -1.781744184024319\n",
       " -2.2373552657361557"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_0 = 0.1\n",
    "nn2 = FastChain(FastDense(1,1), (x, p)->p_0*x)\n",
    "p_1 = initial_params(nn2)\n",
    "u = [1 2 3]\n",
    "print(nn2(u,p_1))\n",
    "\n",
    "\n",
    "\n",
    "## input a vector\n",
    "B = [PolynomialBasis(2),SinBasis(2)]\n",
    "nn_tensor = TensorLayer(B, 1)\n",
    "p_tensor = ones(4)\n",
    "function ff(x)\n",
    "    a = nn_tensor(x,p_tensor)[1]\n",
    "    a\n",
    "end\n",
    "nn_full = FastChain(FastDense(1,2),(x,p)-> ff(x))\n",
    "uu = [1 2 3]\n",
    "p_net = initial_params(nn_full)\n",
    "value = []\n",
    "for u=uu\n",
    "push!(value,nn_full(u, p_net))\n",
    "end\n",
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×3 Matrix{Float64}:\n",
       " 0.0  0.0  0.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_tensor = zeros(2)\n",
    "nn = TensorLayer([SinBasis(2)],1)\n",
    "function f(x)\n",
    "nn(x, p_tensor)[1]\n",
    "end\n",
    "dnn_model = FastChain(FastDense(1,1), (x,p) -> f.(x))\n",
    "u = [1 2 3]\n",
    "dnn_model(u,initial_params(dnn_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_tensor = zeros(4)\n",
    "nn = TensorLayer([SinBasis(2),CosBasis(2)],1)\n",
    "function f(x)\n",
    "nn(x, p_tensor)[1]\n",
    "end\n",
    "dnn_model = FastChain(FastDense(1,2), (x,p) -> f(x))\n",
    "u = [1 2]\n",
    "dnn_model(u,initial_params(dnn_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.5720381313787044]\n",
      "[-1.3034050837082134 -2.606810167416427 -3.91021525112464]\n"
     ]
    }
   ],
   "source": [
    "nn = TensorLayer([SinBasis(2),CosBasis(2)],1)\n",
    "nn_1 = Chain(Dense(1,2),nn)\n",
    "nn_2 = Chain(Dense(1,2),Dense(2,1))\n",
    "u = [1.0 2.0 3.0]\n",
    "println(nn_1(u)) # [-0.02888989486945616]\n",
    "println(nn_2(u)) # [-1.239922964687894 -2.479845929375788 -3.719768894063682]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "MethodError: no method matching (::Dense{typeof(identity), Matrix{Float32}, Vector{Float32}})(::Matrix{Float32}, ::Vector{Float32})\n\u001b[0mClosest candidates are:\n\u001b[0m  (::Dense)(::AbstractVecOrMat{T} where T) at F:\\Code\\Julia\\Julia-1.6.2\\jpkgs\\.julia\\packages\\Flux\\Zz9RI\\src\\layers\\basic.jl:146\n\u001b[0m  (::Dense)(::AbstractArray) at F:\\Code\\Julia\\Julia-1.6.2\\jpkgs\\.julia\\packages\\Flux\\Zz9RI\\src\\layers\\basic.jl:151",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching (::Dense{typeof(identity), Matrix{Float32}, Vector{Float32}})(::Matrix{Float32}, ::Vector{Float32})\n\u001b[0mClosest candidates are:\n\u001b[0m  (::Dense)(::AbstractVecOrMat{T} where T) at F:\\Code\\Julia\\Julia-1.6.2\\jpkgs\\.julia\\packages\\Flux\\Zz9RI\\src\\layers\\basic.jl:146\n\u001b[0m  (::Dense)(::AbstractArray) at F:\\Code\\Julia\\Julia-1.6.2\\jpkgs\\.julia\\packages\\Flux\\Zz9RI\\src\\layers\\basic.jl:151",
      "",
      "Stacktrace:",
      " [1] applychain(fs::Tuple{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}, x::Matrix{Float32}, p::Vector{Float32}) (repeats 2 times)",
      "   @ DiffEqFlux F:\\Code\\Julia\\Julia-1.6.2\\jpkgs\\.julia\\packages\\DiffEqFlux\\N7blG\\src\\fast_layers.jl:20",
      " [2] (::FastChain{Tuple{FastDense{typeof(identity), DiffEqFlux.var\"#initial_params#82\"{Vector{Float32}}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}})(x::Matrix{Int64}, p::Vector{Float32})",
      "   @ DiffEqFlux F:\\Code\\Julia\\Julia-1.6.2\\jpkgs\\.julia\\packages\\DiffEqFlux\\N7blG\\src\\fast_layers.jl:21",
      " [3] top-level scope",
      "   @ In[77]:5",
      " [4] eval",
      "   @ .\\boot.jl:360 [inlined]",
      " [5] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base .\\loading.jl:1116"
     ]
    }
   ],
   "source": [
    "nn = TensorLayer([SinBasis(2),CosBasis(2)],1)\n",
    "p, re = Flux.destructure(Dense(2,1))#(nn)\n",
    "nn_3 = FastChain(FastDense(1,2),re(p))\n",
    "p_0 = initial_params(nn_3)\n",
    "nn_3([1 2 3], p_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Float64}:\n",
       " -1.3789745876764206"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using DiffEqFlux, Flux\n",
    "using Flux: flatten, params\n",
    "x = Array(0:0.1:π)'\n",
    "y = Array(0:0.1:π)'\n",
    "f(a,b) = sin(a)+sin(b)\n",
    "z = f.(x,y)\n",
    "nn = TensorLayer([SinBasis(2),SinBasis(2)],1)\n",
    "nn([1,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Gennerate Data from Lorenz system\n",
    "function lorenz(u,p,t)\n",
    "    x, y, z = u\n",
    "\n",
    "    ẋ = 10.0*(y - x)\n",
    "    ẏ = x*(28.0-z) - y\n",
    "    ż = x*y - (8/3)*z\n",
    "    return [ẋ, ẏ, ż]\n",
    "end\n",
    "\n",
    "u0 = [1.0;0.0;0.0]\n",
    "tspan = (0.0,100.0)\n",
    "dt = 0.1\n",
    "prob = ODEProblem(lorenz,u0,tspan)\n",
    "sol = solve(prob, Tsit5(), saveat = dt, progress = true)\n",
    "data_withoutnois = Array(sol)\n",
    "PolynomialBasis\n",
    "# 2. learn by dnn with a hidden TensorLayer\n",
    "function dnn_output(θ,v)\n",
    "    tensor_layer = TensorLayer([SinBasis(2)], 1)\n",
    "    dnn_model = FastChain(FastDense(1,1),(y,p)-> tensor_layer(y,θ[1:2])[1], FastDense(1, 1))\n",
    "    value = []\n",
    "    for i = v\n",
    "    push!(value,dnn_model(i,θ[3:end])[1])\n",
    "    end\n",
    "    Array(value)\n",
    "end\n",
    "function train(θ)\n",
    "    dnn_output(θ,x)\n",
    "end\n",
    "p = randn(6)\n",
    "println(train(p))  # output a vector.\n",
    "function loss(θ)\n",
    "    pred = train(θ)\n",
    "    sum(abs2, (data .- pred))\n",
    "end\n",
    "println(loss(p)) # 29790\n",
    "res = DiffEqFlux.sciml_train(loss, p, ADAM(0.05), maxiters = 2000)\n",
    "println(loss(res.minimizer)) # 7916"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.2",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
